import os
import subprocess
import glob
import shutil
import csv
import sys
import math
import random
import time
from datetime import datetime

# ================= Configuration =================
# è¾“å‡ºæ–‡ä»¶
REPORT_FILE = "benchmark_results.csv"

# åŸºç¡€é…ç½®
DTYPES = ["float16", "float32"]
P_VALUES = ["1.0", "2.0", "inf", "3.0"]
DATA_RANGE = "S"

# çº¦æŸæ¡ä»¶
MAX_N = 5000
MAX_M = 100000
MAX_COMPLEXITY = 1e12  # N^2 * M

# ================= Utils =================
def clean_logs():
    """Clean old OP* directories"""
    for item in glob.glob("OP*"):
        if os.path.isdir(item):
            shutil.rmtree(item)

def update_config(n, m, p, dtype):
    """Rewrite config.txt"""
    config_content = f"""# Auto-generated by benchmark script
N={n}
M={m}
p={p}
data_type={dtype}
data_range={DATA_RANGE}
"""
    with open("config.txt", "w") as f:
        f.write(config_content)

def get_latest_profile_dir():
    """Get the latest generated OP directory"""
    dirs = glob.glob("OP*")
    if not dirs:
        return None
    dirs.sort(key=os.path.getmtime, reverse=True)
    return dirs[0]

def parse_duration(log_dir):
    """Find 'Task Duration(us)' for Pdist operator"""
    csv_path = None
    for root, dirs, files in os.walk(log_dir):
        if "OpBasicInfo.csv" in files:
            csv_path = os.path.join(root, "OpBasicInfo.csv")
            break
    
    if not csv_path:
        return None

    try:
        with open(csv_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            if reader.fieldnames:
                reader.fieldnames = [name.strip() for name in reader.fieldnames]
            for row in reader:
                op_name = row.get("Op Name", "")
                if "Pdist" in op_name:
                    return float(row.get("Task Duration(us)", 0))
    except Exception:
        pass
    return None

def generate_log_steps(start, end, num_steps, jitter=True):
    """
    Generate logarithmic steps with random jitter to avoid perfect alignment.
    """
    steps = set()
    if start <= 0: start = 1
    
    # Log-space generation
    log_start = math.log(start)
    log_end = math.log(end)
    step_size = (log_end - log_start) / (num_steps - 1)
    
    for i in range(num_steps):
        val = math.exp(log_start + i * step_size)
        val = int(round(val))
        
        # Add jitter: +/- small random amount to hit unaligned numbers
        if jitter and val > 10:
            offset = random.randint(-2, 3) 
            # é¿å…åŠ ä¸Š offset åå˜æˆå¶æ•° (å¦‚æœæˆ‘ä»¬æƒ³åˆ»æ„æµ‹å¥‡æ•°)
            # è¿™é‡Œç®€å•èµ·è§ï¼Œåªè¦ä¸æ˜¯è´Ÿæ•°å°±è¡Œ
            val += offset
        
        if val < 1: val = 1
        steps.add(val)
        
    return sorted(list(steps))

# ================= Test Case Generation =================
def get_test_cases():
    cases = []
    
    # --- Group A: Feature Sensitivity (Fixed N, Scan M) ---
    # N å›ºå®šä¸º 251 (ç´ æ•°, æ¥è¿‘ 256)
    # M ä» 128 æ‰«åˆ° 100,000
    fixed_n = 251
    m_steps = generate_log_steps(128, 100000, 20) # 20 points
    for m in m_steps:
        cases.append({
            "group": "A_Fix_N_Scan_M",
            "n": fixed_n,
            "m": m
        })

    # --- Group B: Batch Sensitivity (Fixed M, Scan N) ---
    # M å›ºå®šä¸º 1021 (ç´ æ•°, æ¥è¿‘ 1024)
    # N ä» 32 æ‰«åˆ° 5000
    fixed_m = 1021
    n_steps = generate_log_steps(32, 5000, 20) # 20 points
    for n in n_steps:
        # å»é‡
        if not any(c['n'] == n and c['m'] == fixed_m for c in cases):
            cases.append({
                "group": "B_Fix_M_Scan_N",
                "n": n,
                "m": fixed_m
            })

    # --- Group C: Proportional Scaling (Rays) ---
    rays = [
        ("C_Scale_1_1", 1.0, 1.0),   # Square
        ("C_Scale_1_4", 1.0, 4.0),   # Wide (M > N)
        ("C_Scale_4_1", 4.0, 1.0)    # Tall (N > M)
    ]
    
    # Base scales for the rays
    base_scales = generate_log_steps(32, 3000, 12) # 12 points per ray
    
    for group_name, r_n, r_m in rays:
        for scale in base_scales:
            n = int(scale * r_n) + random.randint(0, 2)
            m = int(scale * r_m) + random.randint(0, 2)
            
            # Constraints Check
            if n > MAX_N or m > MAX_M:
                continue
            
            # å»é‡
            if not any(c['n'] == n and c['m'] == m for c in cases):
                cases.append({
                    "group": group_name,
                    "n": n,
                    "m": m
                })
    
    # Global Constraints Filter
    valid_cases = []
    for c in cases:
        complexity = float(c['n']) ** 2 * float(c['m'])
        if complexity > MAX_COMPLEXITY:
            continue
        valid_cases.append(c)
        
    return valid_cases

# ================= Main Loop =================
def main():
    # 1. Prepare Cases
    shape_cases = get_test_cases()
    
    # Expand by Dtype and P values
    full_tasks = []
    for shape in shape_cases:
        for dtype in DTYPES:
            for p in P_VALUES:
                task = shape.copy()
                task['dtype'] = dtype
                task['p'] = p
                full_tasks.append(task)

    total_tasks = len(full_tasks)
    print(f"ğŸš€ Benchmarking Plan Loaded.")
    print(f"   Unique Shapes: {len(shape_cases)}")
    print(f"   Total Runs:    {total_tasks}")
    print(f"   Est. Time:     {total_tasks/60:.1f} hours (assuming 1min/case)")
    print("-" * 60)

    # 2. Initialize CSV
    file_exists = os.path.isfile(REPORT_FILE)
    with open(REPORT_FILE, 'a', newline='') as f:
        writer = csv.writer(f)
        if not file_exists:
            writer.writerow(["Timestamp", "Group", "N", "M", "P", "Type", "Duration(us)", "Status"])

    # 3. Execution Loop
    for idx, task in enumerate(full_tasks):
        n = task['n']
        m = task['m']
        p = task['p']
        dtype = task['dtype']
        group = task['group']
        
        prefix = f"[{idx+1}/{total_tasks}]"
        print(f"{prefix} Group:{group:<15} N={n:<5} M={m:<6} P={p:<4} {dtype:<8} ... ", end="", flush=True)
        
        clean_logs()
        update_config(n, m, p, dtype)
        
        status = "OK"
        duration = -1
        
        start_time = time.time()
        try:
            env = os.environ.copy()
            env["ASCEND_DEVICE_ID"] = "7"
            # Run msprof
            # timeout set to 120s to prevent hang
            subprocess.run(["msprof", "op", "./run.sh"], 
                           capture_output=True, 
                           check=True,
                           timeout=120) 
            
            log_dir = get_latest_profile_dir()
            if log_dir:
                d = parse_duration(log_dir)
                if d is not None:
                    duration = d
                else:
                    status = "ParseErr"
            else:
                status = "NoLogDir"

        except subprocess.TimeoutExpired:
            status = "Timeout"
            print("TIMEOUT!", end=" ")
        except subprocess.CalledProcessError:
            status = "RunFail"
        except Exception as e:
            status = "Error"
        
        # Write Result Immediately
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        with open(REPORT_FILE, 'a', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([timestamp, group, n, m, p, dtype, duration, status])
        
        elapsed = time.time() - start_time
        if duration > 0:
            print(f"Done. {duration:.2f} us")
        else:
            print(f"Failed ({status}).")

    print(f"\nâœ… All tests completed. Results saved to {REPORT_FILE}")

if __name__ == "__main__":
    main()