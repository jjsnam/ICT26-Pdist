import os
import subprocess
import glob
import shutil
import csv
import sys
import math
import random
import time
from datetime import datetime

# ================= Configuration =================
# è¾“å‡ºåˆ°ç‹¬ç«‹çš„æ–‡ä»¶ï¼Œé¿å…æ··æ·†ï¼Œåç»­ä½ å¯ä»¥æŠŠä¸¤ä¸ªcsvæ‰‹åŠ¨åˆå¹¶
REPORT_FILE = "benchmark_results_large.csv"

# åŸºç¡€é…ç½®
DTYPES = ["float16", "float32"]
P_VALUES = ["1.0", "2.0", "inf", "3.0"]
DATA_RANGE = "S"

# çº¦æŸæ¡ä»¶ (æ”¾å®½é™åˆ¶ä»¥å…è®¸ 5000x40000)
MAX_N = 6000          # ç¨å¾®æ”¾å®½ä¸€ç‚¹
MAX_M = 200000
MAX_COMPLEXITY = 1.1e12  # N^2 * M, å…è®¸ç¨å¾®è¶…è¿‡ 1e12 ä¸€ç‚¹ç‚¹

# ================= Utils =================
def clean_logs():
    """Clean old OP* directories"""
    for item in glob.glob("OP*"):
        if os.path.isdir(item):
            shutil.rmtree(item)

def update_config(n, m, p, dtype):
    """Rewrite config.txt"""
    config_content = f"""# Auto-generated by benchmark script
N={n}
M={m}
p={p}
data_type={dtype}
data_range={DATA_RANGE}
"""
    with open("config.txt", "w") as f:
        f.write(config_content)

def get_latest_profile_dir():
    """Get the latest generated OP directory"""
    dirs = glob.glob("OP*")
    if not dirs:
        return None
    dirs.sort(key=os.path.getmtime, reverse=True)
    return dirs[0]

def parse_duration(log_dir):
    """Find 'Task Duration(us)' for Pdist operator"""
    csv_path = None
    for root, dirs, files in os.walk(log_dir):
        if "OpBasicInfo.csv" in files:
            csv_path = os.path.join(root, "OpBasicInfo.csv")
            break
    
    if not csv_path:
        return None

    try:
        with open(csv_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            if reader.fieldnames:
                reader.fieldnames = [name.strip() for name in reader.fieldnames]
            for row in reader:
                op_name = row.get("Op Name", "")
                if "Pdist" in op_name:
                    return float(row.get("Task Duration(us)", 0))
    except Exception:
        pass
    return None

# ================= Test Case Generation (Large Scale Focus) =================
def get_large_test_cases():
    cases = []
    
    # å®šä¹‰æˆ‘ä»¬è¦è¡¥å……çš„å¤§æ¯”ä¾‹å°„çº¿
    # (Group Name, N_Ratio, M_Ratio)
    rays = [
        ("C_Scale_1_1", 1.0, 1.0),   # è¡¥å…… 3000 -> 5000
        ("C_Scale_1_4", 1.0, 4.0),   # è¡¥å…… 3000 -> 5000 (M -> 20000)
        ("C_Scale_4_1", 4.0, 1.0),   # è¡¥å…… N -> 5000 (M -> 1250)
        ("C_Scale_1_8", 1.0, 8.0),   # NEW! ä¸“é—¨è¦†ç›– 5000 x 40000
    ]
    
    # åŸºç¡€ Scaleï¼šä» 2500 å¼€å§‹ï¼Œä¸€ç›´æ¨åˆ° 5500
    # æˆ‘ä»¬ç”Ÿæˆ 8 ä¸ªå…³é”®ç‚¹ï¼Œé‡ç‚¹è¦†ç›– 3000, 4000, 5000 é™„è¿‘
    # [2500, 2928, 3428, 4014, 4700, 5500] ç±»ä¼¼çš„å¯¹æ•°åˆ†å¸ƒ
    import numpy as np
    base_scales = np.geomspace(2500, 5200, num=7, dtype=int).tolist()
    
    # é¢å¤–å¼ºåˆ¶åŠ å…¥ 5000 è¿™ä¸ªæ•´æ•°ç‚¹
    if 5000 not in base_scales:
        base_scales.append(5000)
    base_scales.sort()

    for group_name, r_n, r_m in rays:
        for scale in base_scales:
            # è®¡ç®— N å’Œ M
            n = int(scale * r_n)
            m = int(scale * r_m)
            
            # åŠ å…¥ä¸€ç‚¹ç‚¹ Jitter (éšæœºæ‰°åŠ¨)ï¼Œé¿å…å…¨æ˜¯æ•´ç™¾æ•´åƒ
            if n > 100: n += random.randint(-3, 3)
            if m > 100: m += random.randint(-3, 3)
            
            # è¿‡æ»¤æ‰å·²ç»è·‘è¿‡çš„å° Case (N < 2800 ä¸” M < 10000 çš„å¤§æ¦‚ç‡è·‘è¿‡äº†)
            # è¿™é‡Œç®€å•èµ·è§ï¼Œåªä¿ç•™æ¯”è¾ƒå¤§çš„
            # å¯¹äº 4:1 çš„æƒ…å†µï¼ŒM å¾ˆå°ï¼Œæ‰€ä»¥ä¸»è¦çœ‹ N
            if n < 2800 and m < 10000:
                continue

            # Constraints Check
            if n > MAX_N or m > MAX_M:
                continue
            
            # å¤æ‚åº¦æ£€æŸ¥
            complexity = float(n) ** 2 * float(m)
            if complexity > MAX_COMPLEXITY:
                # å¦‚æœè¶…æ ‡äº†ï¼Œæˆ‘ä»¬å°è¯•ç¼©å°ä¸€ç‚¹ç‚¹ä»¥è´´è¿‘è¾¹ç•Œï¼Œè€Œä¸æ˜¯ç›´æ¥ä¸¢å¼ƒ
                # è¿™æ ·èƒ½ä¿è¯æˆ‘ä»¬æµ‹åˆ°æé™
                factor = (MAX_COMPLEXITY / complexity) ** (1/3) # ç²—ç•¥ç¼©æ”¾
                n = int(n * factor)
                m = int(m * factor)
            
            # å†æ¬¡æ£€æŸ¥å»é‡
            if not any(c['n'] == n and c['m'] == m for c in cases):
                cases.append({
                    "group": group_name,
                    "n": n,
                    "m": m
                })

    return cases

# ================= Main Loop =================
def main():
    # 1. Prepare Cases
    shape_cases = get_large_test_cases()
    
    # Expand by Dtype and P values
    full_tasks = []
    for shape in shape_cases:
        for dtype in DTYPES:
            for p in P_VALUES:
                task = shape.copy()
                task['dtype'] = dtype
                task['p'] = p
                full_tasks.append(task)

    total_tasks = len(full_tasks)
    print(f"ğŸš€ Large Scale Benchmark Plan Loaded.")
    print(f"   Focus:         Group C (Proportional Scaling)")
    print(f"   Unique Shapes: {len(shape_cases)}")
    print(f"   Total Runs:    {total_tasks}")
    # å¤§ç®—å­è·‘å¾—æ…¢ï¼Œå‡è®¾æ¯ä¸ª 2 åˆ†é’Ÿ
    print(f"   Est. Time:     {total_tasks * 2 / 60:.1f} hours (assuming 2min/case)")
    print("-" * 60)

    # 2. Initialize CSV
    file_exists = os.path.isfile(REPORT_FILE)
    with open(REPORT_FILE, 'a', newline='') as f:
        writer = csv.writer(f)
        if not file_exists:
            writer.writerow(["Timestamp", "Group", "N", "M", "P", "Type", "Duration(us)", "Status"])

    # 3. Execution Loop
    for idx, task in enumerate(full_tasks):
        n = task['n']
        m = task['m']
        p = task['p']
        dtype = task['dtype']
        group = task['group']
        
        prefix = f"[{idx+1}/{total_tasks}]"
        print(f"{prefix} Group:{group:<15} N={n:<5} M={m:<6} P={p:<4} {dtype:<8} ... ", end="", flush=True)
        
        clean_logs()
        update_config(n, m, p, dtype)
        
        status = "OK"
        duration = -1
        
        start_time = time.time()
        try:
            # ç¯å¢ƒå˜é‡
            env = os.environ.copy()
            # ç¡®ä¿è¿™é‡Œæ˜¯ä½ æƒ³è¦çš„ NPU å¡å·ï¼Œå¦‚æœ run.sh é‡Œæ²¡æ”¹ï¼Œè¿™é‡Œå¯ä»¥å¼ºè¡Œæ³¨å…¥
            env["ASCEND_DEVICE_ID"] = "7" 

            # å¤§ç®—å­å®¹æ˜“è¶…æ—¶ï¼ŒæŠŠ timeout è®¾é•¿ä¸€ç‚¹ (æ¯”å¦‚ 5 åˆ†é’Ÿ = 300s)
            subprocess.run(["msprof", "op", "./run.sh"], 
                           capture_output=True, 
                           check=True,
                           timeout=300,
                           env=env) 
            
            log_dir = get_latest_profile_dir()
            if log_dir:
                d = parse_duration(log_dir)
                if d is not None:
                    duration = d
                else:
                    status = "ParseErr"
            else:
                status = "NoLogDir"

        except subprocess.TimeoutExpired:
            status = "Timeout"
            print("TIMEOUT!", end=" ")
        except subprocess.CalledProcessError:
            status = "RunFail"
        except Exception as e:
            status = "Error"
        
        # Write Result Immediately
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        with open(REPORT_FILE, 'a', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([timestamp, group, n, m, p, dtype, duration, status])
        
        # Print
        if duration > 0:
            # è½¬æ¢ä¸º ms æ˜¾ç¤ºæ›´ç›´è§‚
            print(f"Done. {duration/1000:.2f} ms")
        else:
            print(f"Failed ({status}).")

    print(f"\nâœ… Large scale tests completed. Results saved to {REPORT_FILE}")

if __name__ == "__main__":
    main()